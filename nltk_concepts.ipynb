{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eb58577-4292-4990-89a8-081680050467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0326ad91-474c-4039-9aa7-d51a34999395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing-I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60d76497-a587-4c77-ac67-9b7adb2a373b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Welcome, to Aimen Saeed's NLP practice.\\nPlease do complete this practice! become going home.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus='''Hello Welcome, to Aimen Saeed's NLP practice.\n",
    "Please do complete this practice! become going home.'''\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e21d933-0b15-4455-b08f-44dc08e66df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tokenization\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63fff3b5-0918-4979-b961-02a0d1a35677",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0297cfa7-ba18-4d11-b974-f5dcc61e879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9391a7aa-5b34-47fb-99bc-dd0d0f5eb580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Aimen',\n",
       " 'Saeed',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'practice',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'complete',\n",
       " 'this',\n",
       " 'practice',\n",
       " '!',\n",
       " 'become',\n",
       " 'going',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c559995-3717-46e9-a80a-4ad0cdcf0c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'to', 'Aimen', 'Saeed', \"'s\", 'NLP', 'practice', '.']\n",
      "['Please', 'do', 'complete', 'this', 'practice', '!']\n",
      "['become', 'going', 'home', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentences in documents:\n",
    "    print(word_tokenize(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac8d1106-8ae2-491f-999d-08db6e86d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75222fa9-3978-4f28-bd36-e2ef841302a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Aimen',\n",
       " 'Saeed',\n",
       " \"'\",\n",
       " 's',\n",
       " 'NLP',\n",
       " 'practice',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'complete',\n",
       " 'this',\n",
       " 'practice',\n",
       " '!',\n",
       " 'become',\n",
       " 'going',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)    ## the apostrophe is separated too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c3beaf6-c821-4089-91b1-206c90d1d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8548c5f9-c7c8-4681-8909-c941dd32f880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Aimen',\n",
       " 'Saeed',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'practice.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'complete',\n",
       " 'this',\n",
       " 'practice',\n",
       " '!',\n",
       " 'become',\n",
       " 'going',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=TreebankWordTokenizer()     #full stop is not considered as word except the last one.\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5242fbb-7a24-4fd6-8a25-d8a2921b426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1e2308b-e244-4330-9f96-64762ff22e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#techinques of stemming\n",
    "#1. PorterStemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6083d24-601f-42e4-8de3-d73bf4bc60d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1256366-6920-4f4e-ae58-b8fca3a6ea0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eaten\n",
      "history----->histori\n",
      "programs----->program\n",
      "writing----->write\n",
      "writes----->write\n",
      "programming----->program\n",
      "finalized----->final\n"
     ]
    }
   ],
   "source": [
    "words=['eating','eats','eaten','history','programs','writing','writes','programming','finalized']\n",
    "stemming=PorterStemmer()\n",
    "for word in words:\n",
    "    print(word+'----->'+stemming.stem(word))\n",
    "    \n",
    "#limitation: the meaning of the word may change or itr may unmeaningful. this is fixed with lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd2e4013-5790-4472-84ac-e220ebda0935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. RegexpStemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3e390bf-ad91-4d8f-94f0-9097dc1144c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b30e6f1-cc61-4465-a968-d14d9787d13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingeat'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer=RegexpStemmer('ing$|s$|able$',min=4)\n",
    "reg_stemmer.stem('eating')\n",
    "reg_stemmer.stem('ingeating')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0fc8494-712d-44d0-9302-e49f333a20bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. SnowballStemmer  better than porterstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ffad25d-b046-4119-b1d1-05f4c0ba7b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30a23e36-349b-4ff3-bbe4-bda19e4ec2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eaten\n",
      "history----->histori\n",
      "programs----->program\n",
      "writing----->write\n",
      "writes----->write\n",
      "programming----->program\n",
      "finalized----->final\n"
     ]
    }
   ],
   "source": [
    "snowball=SnowballStemmer('english')\n",
    "for word in words:\n",
    "    print(word+'----->'+snowball.stem(word))         #performs better for other words where porter can't like fairly, sportingly etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1cb201a4-215c-484d-ae3b-d902e617d84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goe'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem('goes')\n",
    "#This problem is solved with lemmatization as it has dictionary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30432552-ed1f-449d-a6c3-f351e36e32c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "#lemmatization--> getting the root word   Vs stemming---->getting the word stem\n",
    "#usecases: Q&A chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9eb08e83-cbf8-4ba1-a554-2d93e64e1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#technique: Wordnet Lemmatizer. it uses the morphy() function to the WordNet CorpusReader class to find a lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a151d42c-1024-4983-a0ef-241c9cd99996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b0093c3-66b9-4958-bd2c-e4067ac4e2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairly'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('fairly',pos='v')     #default POS=n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f190f1a-6dfa-4dc5-a98c-2d783c5ca7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eat\n",
      "history----->history\n",
      "programs----->program\n",
      "writing----->write\n",
      "writes----->write\n",
      "programming----->program\n",
      "finalized----->finalize\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+'----->'+lemmatizer.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e5d8eeb2-caac-4e6f-b23e-e4c2ec01472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "608deb0e-07c3-4bfc-808a-8877adc6e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=''' Mr. President (Quaid-e-Azam Mohammad Ali Jinnah): Ladies and Gentlemen, I cordially thank you, with the utmost sincerity, for the honour you have conferred upon me — the greatest honour that it is possible for this Sovereign Assembly to confer — by electing me as your first President.\n",
    "I also thank those leaders who have spoken in appreciation of my services and their personal references to me.\n",
    "I sincerely hope that with your support and your co-operation we shall make this Constituent Assembly an example to the world.\n",
    "The Constituent Assembly has got two main functions to perform. \n",
    "The first is the very onerous and responsible task of framing our future constitution of Pakistan and the second of functioning as a full and complete Sovereign body as the Federal Legislature of Pakistan.\n",
    "We have to do the best we can in adopting a provisional constitution for the Federal Legislature of Pakistan.\n",
    "You know really that not only we ourselves are wondering but, I think, the whole world is wondering at this unprecedented cyclonic revolution which has brought about the plan of creating and establishing two independent Sovereign Dominions in this sub-continent.\n",
    "As it is, it has been unprecedented; \n",
    "there is no parallel in the history of the world.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "772240dc-3890-4e93-bd25-6fee3bbdda12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4c77182f-eb7c-4012-9c72-239fb98aa71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')         #you can create your own stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3ba1a00d-bec8-4bcb-977b-2759ea3c66f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem=PorterStemmer()\n",
    "sen=nltk.sent_tokenize(paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6e393b7e-039b-4af1-8c3e-5ccd6a609f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply stopwords and filter and then apply stemming\n",
    "for i in range(len(sen)):\n",
    "    words=nltk.word_tokenize(sen[i])\n",
    "    words=[stem.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sen[i]=' '.join(words) #converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "055b6953-90d2-4faa-ade9-5d01517e9c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mr. presid ( quaid-e-azam mohammad ali jinnah ) : ladi gentlemen , i cordial thank , utmost sincer , honour confer upon — greatest honour possibl sovereign assembl confer — elect first presid .',\n",
       " 'i also thank leader spoken appreci servic person refer .',\n",
       " 'i sincer hope support co-oper shall make constitu assembl exampl world .',\n",
       " 'the constitu assembl got two main function perform .',\n",
       " 'the first oner respons task frame futur constitut pakistan second function full complet sovereign bodi feder legislatur pakistan .',\n",
       " 'we best adopt provision constitut feder legislatur pakistan .',\n",
       " 'you know realli wonder , i think , whole world wonder unpreced cyclon revolut brought plan creat establish two independ sovereign dominion sub-contin .',\n",
       " 'as , unpreced ; parallel histori world .']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0c565710-0379-4546-8ba3-7468f15bd82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#snowball stemming on the pargraph\n",
    "snowballstem=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "52be16b1-d077-4325-bad6-18488054d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sen)):\n",
    "    words=nltk.word_tokenize(sen[i])\n",
    "    words=[snowballstem.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sen[i]=' '.join(words) #converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e7e84cf6-860b-407c-9ff3-b7036da58085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mr. presid ( quaid-e-azam mohammad ali jinnah ) : ladi gentlemen , cordial thank , utmost sincer , honour confer upon — greatest honour possibl sovereign assembl confer — elect first presid .',\n",
       " 'also thank leader spoken appreci servic person refer .',\n",
       " 'sincer hope support co-op shall make constitu assembl exampl world .',\n",
       " 'constitu assembl got two main function perform .',\n",
       " 'first oner respon task frame futur constitut pakistan second function full complet sovereign bodi feder legislatur pakistan .',\n",
       " 'best adopt provi constitut feder legislatur pakistan .',\n",
       " 'know realli wonder , think , whole world wonder unprec cyclon revolut brought plan creat establish two independ sovereign dominion sub-contin .',\n",
       " ', unprec ; parallel histori world .']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "668c120b-e3ff-432c-8b34-6b111bcddab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization\n",
    "lem=WordNetLemmatizer()\n",
    "for i in range(len(sen)):\n",
    "    words=nltk.word_tokenize(sen[i])\n",
    "    words=[lem.lemmatize(word,pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sen[i]=' '.join(words) #converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1e131b0f-a83e-41c2-9a75-68863b8370cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mr. presid ( quaid-e-azam mohammad ali jinnah ) : ladi gentleman , cordial thank , utmost sincer , honour confer upon — greatest honour possibl sovereign assembl confer — elect first presid .',\n",
       " 'also thank leader speak appreci servic person refer .',\n",
       " 'sincer hope support co-op shall make constitu assembl exampl world .',\n",
       " 'constitu assembl get two main function perform .',\n",
       " 'first oner respon task frame futur constitut pakistan second function full complet sovereign bodi feder legislatur pakistan .',\n",
       " 'best adopt provi constitut feder legislatur pakistan .',\n",
       " 'know realli wonder , think , whole world wonder unprec cyclon revolut bring plan creat establish two independ sovereign dominion sub-contin .',\n",
       " ', unprec ; parallel histori world .']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b69dfe-b7bb-4968-b11a-fb319786330e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
